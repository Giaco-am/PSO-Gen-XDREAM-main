{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "from src.xdream.core.generator import DeePSiMGenerator\n",
    "from src.xdream.core.optimizer import CMAESOptimizer, GeneticOptimizer, HybridOptimizer\n",
    "from src.xdream.core.scorer import ActivityScorer\n",
    "from src.xdream.core.subject import TorchNetworkSubject\n",
    "from src.xdream.core.utils.dataset import MiniImageNet, NaturalStimuliLoader\n",
    "from src.xdream.core.utils.io_ import read_json\n",
    "from src.xdream.core.utils.logger import LoguruLogger\n",
    "from src.xdream.core.utils.probe import RecordingProbe\n",
    "from src.xdream.core.utils.torch_net_load_functs import robustBench_load, torch_load\n",
    "lsettings_fp = '/home/gamerio/Documents/opt/PSO-Gen-XDREAM-main/src/xdream/experiment/local_settings.json'\n",
    "local_setting = read_json(lsettings_fp)\n",
    "\n",
    "OUT_DIR         : str = local_setting.get('out_dir',      None)\n",
    "WEIGHTS         : str = local_setting.get('weights',      None)\n",
    "DATASET         : str = local_setting.get('dataset',      None)\n",
    "\n",
    "CUSTOM_WEIGHTS  : str = local_setting.get('custom_weights',  None)\n",
    "device = 'cuda'\n",
    "PARAM_net_name = 'resnet50'\n",
    "path2CustomW = ''#'Liu2023Comprehensive_ConvNeXt-B'#os.path.join(CUSTOM_WEIGHTS, PARAM_net_name, 'imagenet_l2_3_0.pt')\n",
    "\n",
    "generator = DeePSiMGenerator(\n",
    "    root    = str(WEIGHTS),\n",
    "    variant = str('fc7') # type: ignore\n",
    ").to(device)\n",
    "\n",
    "# Create a on-the-fly network subject to extract all network layer names\n",
    "layer_info: Dict[str, Tuple[int, ...]] = TorchNetworkSubject(\n",
    "    network_name=str(PARAM_net_name)\n",
    ").layer_info\n",
    "\n",
    "# Probe\n",
    "record_target = {'56_linear_01': [0]}\n",
    "probe = RecordingProbe(target = record_target) # type: ignore\n",
    "\n",
    "# Subject with attached recording probe\n",
    "sbj_net = TorchNetworkSubject(\n",
    "    record_probe=probe,\n",
    "    network_name=PARAM_net_name,\n",
    "    t_net_loading = torch_load,\n",
    "    custom_weights_path = path2CustomW\n",
    ")\n",
    "\n",
    "# Set the network in evaluation mode\n",
    "sbj_net.eval()\n",
    "\n",
    "scoring_units = record_target\n",
    "scorer = ActivityScorer(\n",
    "    scoring_units=scoring_units,\n",
    "    units_reduction='mean',  # type: ignore\n",
    "    layer_reduction='mean', # type: ignore\n",
    ")\n",
    "n_iter = 500\n",
    "pso_params = {\n",
    "    'inertia_max': 0.99,\n",
    "    'inertia_min': 0.95,\n",
    "    'cognitive': 2.5,\n",
    "    'social': 2,\n",
    "    'v_clip': 0.15,\n",
    "    'num_informants': 15,\n",
    "   \n",
    "    'max_iterations' :n_iter,\n",
    "    #'first_PSO_interval' : 0.6,\n",
    "    #'second_PSO_interval' : 0.9,  \n",
    "}\n",
    "    \n",
    "\n",
    "\n",
    "#--- OPTIMIZER ---\n",
    "optim = HybridOptimizer(\n",
    "            codes_shape=generator.input_dim,\n",
    "            pop_size=50,    \n",
    "            rnd_seed=50000,     \n",
    "            pso_params=pso_params,        \n",
    "            max_iterations=n_iter,       \n",
    "            diversity_threshold=0.01,\n",
    "            stagnation_threshold=0.01,\n",
    "            first_PSO_interval = 0.6,\n",
    "            second_PSO_interval= 0.9\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "#optim = CMAESOptimizer(\n",
    "#     codes_shape = generator.input_dim,\n",
    "#     rnd_seed    = 50000,\n",
    "#     pop_size    = 50,\n",
    "#     sigma0      = 1.0,\n",
    "#)\n",
    "#optim = GeneticOptimizer(\n",
    "#    codes_shape  = generator.input_dim,\n",
    "#    rnd_seed     = 50000,\n",
    "#    pop_size     = 50,\n",
    "#    rnd_scale    = 1,\n",
    "#    mut_size     = 0.3,\n",
    "#    mut_rate     = 0.3,\n",
    "#    allow_clones = True,\n",
    "#    n_parents    = 4\n",
    "#)\n",
    "\n",
    "\n",
    "logger = LoguruLogger(to_file=False)\n",
    "\n",
    "\n",
    "template = [True]\n",
    "use_nat  = template.count(False) > 0\n",
    "\n",
    "# Create dataset and loader\n",
    "dataset  = MiniImageNet(root=DATASET) #RandomImageDataset(n_img=100, img_size=(224, 224,))# \n",
    "\n",
    "nat_img_loader   = NaturalStimuliLoader(\n",
    "    dataset=dataset,\n",
    "    template=template,\n",
    "    shuffle=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"dataset\"      : dataset if use_nat else None,\n",
    "    \"render\"       : False,\n",
    "    \"close_screen\" : True,\n",
    "    \"use_nat\"      : use_nat\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xdream.experiment.maximize_activity import MaximizeActivityExperiment\n",
    "\n",
    "\n",
    "MA = MaximizeActivityExperiment(generator,\n",
    "                           sbj_net,\n",
    "                           scorer,\n",
    "                           optim,\n",
    "                           n_iter,\n",
    "                           logger,\n",
    "                           nat_img_loader,\n",
    "\n",
    "                           data,\n",
    "                           name = 'maximize_activity')\n",
    "\n",
    "\n",
    "msg = MA.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS AND ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.path.expanduser('/home/gamerio/Desktop/hybridoutput/MaximizeActivity/27_06_optvars_highGenetic_lowfisrtPSOinterval_lesssamples/data.pkl')\n",
    "file_path = os.path.expanduser('/home/gamerio/Desktop/hybridoutput/MaximizeActivity/1_07_optvars_final/data.pkl')\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# The structure of `data` is :\n",
    "# data['optim_scores'][unit][optimizer_type] = {\n",
    "#     'mean': (array of shape [n_repetitions, n_iterations]),\n",
    "#     'best': (array of shape [n_repetitions, n_iterations]),\n",
    "#     'elapsed_time': (list or array of times, one per repetition)\n",
    "# }\n",
    "\n",
    "units = list(data['optim_scores'].keys())\n",
    "optimizers = ['genetic', 'hybrid', 'cmaes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_mean(opttype):\n",
    "    \"\"\"\n",
    "    Computes the average \"mean scores\" across all units at each iteration, \n",
    "    as well as the corresponding SEM (standard error of the mean).\n",
    "    \"\"\"\n",
    "    averaged_list = []\n",
    "    for unit in units:\n",
    "        # shape: [n_repetitions, n_iterations]\n",
    "        arr = data['optim_scores'][unit][opttype]['mean']\n",
    "        # mean across repeated runs at each iteration\n",
    "        mean_per_iter = np.mean(arr, axis=0)\n",
    "        averaged_list.append(mean_per_iter)\n",
    "    averaged_list = np.array(averaged_list)  # shape: [n_units, n_iterations]\n",
    "    \n",
    "    g_mean = np.mean(averaged_list, axis=0)\n",
    "    g_sem = np.std(averaged_list, axis=0) / np.sqrt(averaged_list.shape[0])\n",
    "    return g_mean, g_sem\n",
    "\n",
    "def best_final_score(unit, opttype):\n",
    "    \"\"\"\n",
    "    Returns the best final score for a given unit and optimizer.\n",
    "    'best' has shape [n_repetitions, n_iterations], so we take max over all reps and all iterations.\n",
    "    \"\"\"\n",
    "    return np.max(data['optim_scores'][unit][opttype]['best'])\n",
    "\n",
    "def average_time(unit, opttype):\n",
    "    \"\"\"\n",
    "    Returns the average elapsed time over all runs for a given unit and optimizer.\n",
    "    \"\"\"\n",
    "    return np.mean(data['optim_scores'][unit][opttype]['elapsed_time'])\n",
    "\n",
    "def score_time_ratio(unit, opttype):\n",
    "    \"\"\"\n",
    "    Returns the ratio of final best score to average time (a simple measure of \n",
    "    'score per second' for each unit and optimizer).\n",
    "    \"\"\"\n",
    "    return best_final_score(unit, opttype) / average_time(unit, opttype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_times_dict = {\n",
    "    opt: [average_time(u, opt) for u in units] for opt in optimizers\n",
    "}\n",
    "\n",
    "# Global means (and SEM) across iterations\n",
    "means_and_sems = {\n",
    "    opt: global_mean(opt) for opt in optimizers\n",
    "}\n",
    "\n",
    "iterations = np.arange(0, 500, 1)\n",
    "\n",
    "# Per-unit maximum and mean\n",
    "max_scores_dict = { \n",
    "    opt: [ np.max(data['optim_scores'][u][opt]['best']) for u in units ] \n",
    "    for opt in optimizers\n",
    "}\n",
    "mean_scores_dict = {\n",
    "    opt: [ np.mean(data['optim_scores'][u][opt]['mean']) for u in units ]\n",
    "    for opt in optimizers\n",
    "}\n",
    "\n",
    "# Final best score for each unit and each optimizer\n",
    "final_best_scores = {\n",
    "    opt: [best_final_score(u, opt) for u in units] for opt in optimizers\n",
    "}\n",
    "\n",
    "# Score/time ratio\n",
    "score_time_ratios = {\n",
    "    opt: [ score_time_ratio(u, opt) for u in units ] for opt in optimizers\n",
    "}\n",
    "\n",
    "mean_score_time_ratios = {\n",
    "    opt: [mean_scores_dict[opt][i] / elapsed_times_dict[opt][i] for i in range(len(units))]\n",
    "    for opt in optimizers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use a pleasant style and context\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "\n",
    "# Define a nice color palette (e.g., ColorBrewer Set1)\n",
    "colors = sns.color_palette(\"Set1\", n_colors=len(optimizers))  # expecting optimizers = ['genetic', 'hybrid', 'cmaes']\n",
    "\n",
    "###############################################################################\n",
    "# FIRST FIGURE: 4 plots (2x2) with improved spacing and aesthetics\n",
    "###############################################################################\n",
    "fig1, axs1 = plt.subplots(2, 2, figsize=(20, 12))\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Subplot (1,1): Boxplot of elapsed times for each optimizer\n",
    "ax = axs1[0, 0]\n",
    "all_times = [elapsed_times_dict[opt] for opt in optimizers]\n",
    "bp = ax.boxplot(all_times, labels=optimizers, patch_artist=True, widths=0.6)\n",
    "\n",
    "# Loop over boxes and set face/edge colors from our palette:\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor(color)\n",
    "for median in bp['medians']:\n",
    "    median.set_color('black')\n",
    "\n",
    "ax.set_xlabel('Optimizer')\n",
    "ax.set_ylabel('Elapsed Time (s)')\n",
    "ax.set_title('Elapsed Time Distribution by Optimizer', fontsize=16)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Subplot (1,2): Global mean scores vs iterations\n",
    "ax = axs1[0, 1]\n",
    "for opt, color in zip(optimizers, colors):\n",
    "    g_mean, g_sem = means_and_sems[opt]\n",
    "    ax.plot(iterations, g_mean, label=opt.capitalize(), color=color, linewidth=2)\n",
    "    ax.fill_between(iterations, g_mean - g_sem, g_mean + g_sem, color=color, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Iterations', fontsize=14)\n",
    "ax.set_ylabel('Average Mean Score', fontsize=14)\n",
    "ax.set_title('Global Mean Score Across 500 Iterations', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Subplot (2,1): Maximum Score by Unit\n",
    "ax = axs1[1, 0]\n",
    "for opt, color in zip(optimizers, colors):\n",
    "    ax.plot(units, max_scores_dict[opt], label=opt.capitalize(), color=color, marker='o', markersize=8, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Unit', fontsize=14)\n",
    "ax.set_ylabel('Maximum Score', fontsize=14)\n",
    "ax.set_title('Maximum Score by Unit and Optimizer', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Subplot (2,2): Mean Score by Unit\n",
    "ax = axs1[1, 1]\n",
    "for opt, color in zip(optimizers, colors):\n",
    "    ax.plot(units, mean_scores_dict[opt], label=opt.capitalize(), color=color, marker='o', markersize=8, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Unit', fontsize=14)\n",
    "ax.set_ylabel('Mean Score', fontsize=14)\n",
    "ax.set_title('Mean Score by Unit and Optimizer', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# SECOND FIGURE: 2 plots (1x2) with improved spacing and aesthetics\n",
    "###############################################################################\n",
    "fig2, axs2 = plt.subplots(1, 2, figsize=(20, 8))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Set up x positions for bar charts:\n",
    "x = np.arange(len(units))\n",
    "width = 0.25\n",
    "\n",
    "# Subplot (1): Final Best Scores by Unit\n",
    "ax = axs2[0]\n",
    "for i, (opt, color) in enumerate(zip(optimizers, colors)):\n",
    "    ax.bar(x + (i - 1) * width, final_best_scores[opt], width,\n",
    "           label=opt.capitalize(), color=color, alpha=0.9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(units, rotation=45, fontsize=12)\n",
    "ax.set_ylabel('Final Best Score', fontsize=14)\n",
    "ax.set_title('Final Best Scores by Unit', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Subplot (2): Score/time ratio by Unit\n",
    "ax = axs2[1]\n",
    "for i, (opt, color) in enumerate(zip(optimizers, colors)):\n",
    "    ax.bar(x + (i - 1) * width, score_time_ratios[opt], width,\n",
    "           label=opt.capitalize(), color=color, alpha=0.9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(units, rotation=45, fontsize=12)\n",
    "ax.set_ylabel('Final Score / Avg Time', fontsize=14)\n",
    "ax.set_title('Score-per-Second Ratio by Unit', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = []\n",
    "for i, unit in enumerate(units):\n",
    "    # final_best_scores[opt][i] is the final best for that unit\n",
    "    best_vals = {opt: final_best_scores[opt][i] for opt in optimizers}\n",
    "    # get the optimizer with the maximum final best\n",
    "    winner_opt = max(best_vals, key=best_vals.get)\n",
    "    winners.append((unit, winner_opt, best_vals[winner_opt]))\n",
    "\n",
    "print(\"Unit  |  Best Optimizer  |  Best Final Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "for (unit, opt, score) in winners:\n",
    "    print(f\"{unit:5s} | {opt:15s} | {score: .3f}\")\n",
    "\n",
    "# We could also examine the \"winning\" frequency:\n",
    "from collections import Counter\n",
    "counter = Counter([w[1] for w in winners])\n",
    "print(\"\\nNumber of wins by each optimizer:\")\n",
    "for opt in optimizers:\n",
    "    print(f\"{opt.capitalize()}: {counter[opt]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID OPTIMIZER PARAMETER SETS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Load the hybrid parameter experiment data\n",
    "\n",
    "param_exp_file_path = os.path.expanduser('/home/gamerio/Desktop/hybridoutput/MaximizeActivity/2_07_optparams_varyingGen/data.pkl')\n",
    "\n",
    "try:\n",
    "    with open(param_exp_file_path, 'rb') as f:\n",
    "        param_data = pickle.load(f)\n",
    "    \n",
    "    # The structure of `param_data` is:\n",
    "    # param_data['optim_scores'][layer][param_set] = {\n",
    "    #     'mean_gens': (array of mean scores across generations),\n",
    "    #     'best_gens': (array of best scores across generations), \n",
    "    #     'elapsed_time': (list of elapsed times),\n",
    "    #     'params': (dict of actual parameter values used)\n",
    "    # }\n",
    "    \n",
    "    # Extract available layers and parameter sets\n",
    "    layers = list(param_data['optim_scores'].keys())\n",
    "    print(f\"Available layers: {layers}\")\n",
    "    \n",
    "    # Focus on the first layer (assuming single layer analysis)\n",
    "    target_layer = layers[0] if layers else None\n",
    "    \n",
    "    if target_layer:\n",
    "        param_sets = list(param_data['optim_scores'][target_layer].keys())\n",
    "        print(f\"Parameter sets tested: {param_sets}\")\n",
    "        \n",
    "        # Set up plotting style\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_context(\"talk\", font_scale=1.0)\n",
    "        colors = sns.color_palette(\"Set2\", n_colors=len(param_sets))\n",
    "        \n",
    "        # =============================================================================\n",
    "        # PARAMETER COMPARISON PLOTS\n",
    "        # =============================================================================\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 3, figsize=(36, 16))\n",
    "        plt.suptitle('Hybrid Optimizer Parameter Set Comparison', fontsize=20, y=0.98)\n",
    "        \n",
    "        # Plot 1: Best scores evolution across generations\n",
    "        ax = axs[0, 0]\n",
    "        for param_set, color in zip(param_sets, colors):\n",
    "            best_gens = param_data['optim_scores'][target_layer][param_set]['best_gens']\n",
    "            if len(best_gens) > 0:\n",
    "                # Convert to numpy array and handle multiple runs\n",
    "                best_array = np.array(best_gens)\n",
    "                if best_array.ndim > 1:\n",
    "                    mean_best = np.mean(best_array, axis=0)\n",
    "                    std_best = np.std(best_array, axis=0)\n",
    "                    iterations = np.arange(len(mean_best))\n",
    "                    ax.plot(iterations, mean_best, label=f'{param_set}', color=color, linewidth=2)\n",
    "                    ax.fill_between(iterations, mean_best - std_best, mean_best + std_best, \n",
    "                                   color=color, alpha=0.2)\n",
    "                else:\n",
    "                    iterations = np.arange(len(best_array))\n",
    "                    ax.plot(iterations, best_array, label=f'{param_set}', color=color, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Best Score')\n",
    "        ax.set_title('Best Score Evolution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Mean scores evolution across generations  \n",
    "        ax = axs[0, 1]\n",
    "        for param_set, color in zip(param_sets, colors):\n",
    "            mean_gens = param_data['optim_scores'][target_layer][param_set]['mean_gens']\n",
    "            if len(mean_gens) > 0:\n",
    "                mean_array = np.array(mean_gens)\n",
    "                if mean_array.ndim > 1:\n",
    "                    mean_mean = np.mean(mean_array, axis=0)\n",
    "                    std_mean = np.std(mean_array, axis=0)\n",
    "                    iterations = np.arange(len(mean_mean))\n",
    "                    ax.plot(iterations, mean_mean, label=f'{param_set}', color=color, linewidth=2)\n",
    "                    ax.fill_between(iterations, mean_mean - std_mean, mean_mean + std_mean,\n",
    "                                   color=color, alpha=0.2)\n",
    "                else:\n",
    "                    iterations = np.arange(len(mean_array))\n",
    "                    ax.plot(iterations, mean_array, label=f'{param_set}', color=color, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Mean Score')\n",
    "        ax.set_title('Mean Score Evolution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Final best scores comparison\n",
    "        ax = axs[0, 2]\n",
    "        final_best_scores = []\n",
    "        param_labels = []\n",
    "        for param_set in param_sets:\n",
    "            best_gens = param_data['optim_scores'][target_layer][param_set]['best_gens']\n",
    "            if len(best_gens) > 0:\n",
    "                best_array = np.array(best_gens)\n",
    "                if best_array.ndim > 1:\n",
    "                    final_scores = np.max(best_array, axis=1)  # Max score per run\n",
    "                    final_best_scores.extend(final_scores)\n",
    "                    param_labels.extend([param_set] * len(final_scores))\n",
    "                else:\n",
    "                    final_best_scores.append(np.max(best_array))\n",
    "                    param_labels.append(param_set)\n",
    "        \n",
    "        # Create boxplot\n",
    "        unique_params = list(set(param_labels))\n",
    "        grouped_scores = [[] for _ in unique_params]\n",
    "        for score, label in zip(final_best_scores, param_labels):\n",
    "            idx = unique_params.index(label)\n",
    "            grouped_scores[idx].append(score)\n",
    "        \n",
    "        bp = ax.boxplot(grouped_scores, labels=unique_params, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(unique_params)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Final Best Score')\n",
    "        ax.set_title('Final Best Score Distribution')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Elapsed time comparison\n",
    "        ax = axs[1, 0] \n",
    "        elapsed_times = []\n",
    "        param_labels_time = []\n",
    "        for param_set in param_sets:\n",
    "            times = param_data['optim_scores'][target_layer][param_set]['elapsed_time']\n",
    "            if len(times) > 0:\n",
    "                elapsed_times.extend(times)\n",
    "                param_labels_time.extend([param_set] * len(times))\n",
    "        \n",
    "        # Create boxplot for times\n",
    "        grouped_times = [[] for _ in unique_params]\n",
    "        for time_val, label in zip(elapsed_times, param_labels_time):\n",
    "            idx = unique_params.index(label)\n",
    "            grouped_times[idx].append(time_val)\n",
    "        \n",
    "        bp = ax.boxplot(grouped_times, labels=unique_params, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(unique_params)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Elapsed Time (s)')\n",
    "        ax.set_title('Runtime Comparison')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Score per second ratio\n",
    "        ax = axs[1, 1]\n",
    "        score_time_ratios = []\n",
    "        param_labels_ratio = []\n",
    "        \n",
    "        for param_set in param_sets:\n",
    "            best_gens = param_data['optim_scores'][target_layer][param_set]['best_gens']\n",
    "            times = param_data['optim_scores'][target_layer][param_set]['elapsed_time']\n",
    "            \n",
    "            if len(best_gens) > 0 and len(times) > 0:\n",
    "                best_array = np.array(best_gens)\n",
    "                if best_array.ndim > 1:\n",
    "                    final_scores = np.max(best_array, axis=1)\n",
    "                    ratios = final_scores / np.array(times)\n",
    "                else:\n",
    "                    ratios = [np.max(best_array) / times[0]] if times else []\n",
    "                \n",
    "                score_time_ratios.extend(ratios)\n",
    "                param_labels_ratio.extend([param_set] * len(ratios))\n",
    "        \n",
    "        # Create boxplot for ratios\n",
    "        grouped_ratios = [[] for _ in unique_params]\n",
    "        for ratio, label in zip(score_time_ratios, param_labels_ratio):\n",
    "            idx = unique_params.index(label)\n",
    "            grouped_ratios[idx].append(ratio)\n",
    "        \n",
    "        bp = ax.boxplot(grouped_ratios, labels=unique_params, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(unique_params)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Score / Time')\n",
    "        ax.set_title('Efficiency (Score per Second)')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Parameter comparison heatmap\n",
    "        ax = axs[1, 2]\n",
    "        \n",
    "        # Extract parameter values for heatmap\n",
    "        param_matrix = []\n",
    "        param_names = []\n",
    "        \n",
    "        for param_set in param_sets:\n",
    "            params = param_data['optim_scores'][target_layer][param_set]['params']\n",
    "            if param_names == []:  # First iteration\n",
    "                param_names = list(params.keys())\n",
    "            param_values = [params.get(name, 0) for name in param_names]\n",
    "            param_matrix.append(param_values)\n",
    "        \n",
    "        if param_matrix:\n",
    "            param_matrix = np.array(param_matrix)\n",
    "            im = ax.imshow(param_matrix, cmap='viridis', aspect='auto')\n",
    "            \n",
    "            ax.set_xticks(range(len(param_names)))\n",
    "            ax.set_xticklabels(param_names, rotation=45, ha='right')\n",
    "            ax.set_yticks(range(len(param_sets)))\n",
    "            ax.set_yticklabels(param_sets)\n",
    "            ax.set_title('Parameter Values Heatmap')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "            # Add text annotations\n",
    "            for i in range(len(param_sets)):\n",
    "                for j in range(len(param_names)):\n",
    "                    text = ax.text(j, i, f'{param_matrix[i, j]:.2f}',\n",
    "                                 ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # SUMMARY STATISTICS AND WINNER ANALYSIS\n",
    "        # =============================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HYBRID OPTIMIZER PARAMETER SET ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Calculate average performance metrics\n",
    "        performance_summary = {}\n",
    "        for param_set in param_sets:\n",
    "            best_gens = param_data['optim_scores'][target_layer][param_set]['best_gens']\n",
    "            times = param_data['optim_scores'][target_layer][param_set]['elapsed_time']\n",
    "            \n",
    "            if len(best_gens) > 0:\n",
    "                best_array = np.array(best_gens)\n",
    "                if best_array.ndim > 1:\n",
    "                    avg_final_score = np.mean(np.max(best_array, axis=1))\n",
    "                    std_final_score = np.std(np.max(best_array, axis=1))\n",
    "                else:\n",
    "                    avg_final_score = np.max(best_array)\n",
    "                    std_final_score = 0\n",
    "                \n",
    "                avg_time = np.mean(times) if times else 0\n",
    "                efficiency = avg_final_score / avg_time if avg_time > 0 else 0\n",
    "                \n",
    "                performance_summary[param_set] = {\n",
    "                    'avg_final_score': avg_final_score,\n",
    "                    'std_final_score': std_final_score,\n",
    "                    'avg_time': avg_time,\n",
    "                    'efficiency': efficiency\n",
    "                }\n",
    "        \n",
    "        # Print summary table\n",
    "        print(f\"{'Parameter Set':<15} {'Avg Final Score':<18} {'Std Final Score':<18} {'Avg Time (s)':<15} {'Efficiency':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        for param_set, metrics in performance_summary.items():\n",
    "            print(f\"{param_set:<15} {metrics['avg_final_score']:<18.3f} {metrics['std_final_score']:<18.3f} \"\n",
    "                  f\"{metrics['avg_time']:<15.2f} {metrics['efficiency']:<12.4f}\")\n",
    "        \n",
    "        # Find the best parameter set by different criteria\n",
    "        print(f\"\\n{'WINNERS BY CRITERIA:':<50}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        best_score_set = max(performance_summary.keys(), \n",
    "                           key=lambda x: performance_summary[x]['avg_final_score'])\n",
    "        print(f\"Best Average Final Score: {best_score_set} ({performance_summary[best_score_set]['avg_final_score']:.3f})\")\n",
    "        \n",
    "        fastest_set = min(performance_summary.keys(),\n",
    "                         key=lambda x: performance_summary[x]['avg_time'])\n",
    "        print(f\"Fastest Runtime: {fastest_set} ({performance_summary[fastest_set]['avg_time']:.2f}s)\")\n",
    "        \n",
    "        most_efficient_set = max(performance_summary.keys(),\n",
    "                               key=lambda x: performance_summary[x]['efficiency'])\n",
    "        print(f\"Most Efficient (Score/Time): {most_efficient_set} ({performance_summary[most_efficient_set]['efficiency']:.4f})\")\n",
    "        \n",
    "        # Show parameter details for the best performing set\n",
    "        print(f\"\\nBEST PARAMETER SET DETAILS ({best_score_set}):\")\n",
    "        print(\"-\" * 50)\n",
    "        best_params = param_data['optim_scores'][target_layer][best_score_set]['params']\n",
    "        for param_name, param_value in best_params.items():\n",
    "            print(f\"{param_name:<25}: {param_value}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No data available in the pickle file.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {param_exp_file_path}\")\n",
    "    print(\"Please check the path and make sure the experiment has been run.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psoXdream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
